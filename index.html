<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Anonymized project page of ALaRM for review.">
  <meta name="keywords" content="alarm, hierarchical, rlhf">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ALaRM: Align Language Models via Hierarchical Rewards Modeling</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-X1M114XVB7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-X1M114XVB7');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/alarm-clock.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="http://www.fudan-disc.com/">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://somelvlm.github.io/">
            SoMeLVLM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dnerf">ALaRM</span>: Align Language Models via Hierarchical Rewards Modeling</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://halfrot.github.io/">Yuhang Lai</a>,&nbsp;</span>
            <span class="author-block">
              <a href="https://siyuanwangw.github.io/">Siyuan Wang</a>,&nbsp;</span>
            <span class="author-block">
              <a href="https://lsjlsj35.github.io/">Shujun Liu</a>,
            </span>
            <br/>
            <span class="author-block">
              <a href="https://xuanjing-huang.github.io/">Xuanjing Huang</a>,&nbsp;
            </span>
            <span class="author-block">
              <a href="http://www.fudan-disc.com/people/zywei">Zhongyu Wei</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Fudan University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.06754"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/halfrot/ALaRM"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">ALaRM</span> is a reinforcement learning framework for enhancing LLM alignment that:
        <br>
        (1) integrates holistic and aspect-specific rewards hierarchically,
        (2) employs a selection and combination methodology based on reward consistency,
        (3) demonstrates effectiveness in long-form question answering and machine translation.
      </h2>
      <img src="static/images/framework.svg">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <span class="dnerf">ALaRM</span>, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences.
          </p>
          <p>
            The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment.
          </p>
          <p>
            We validate our approach through applications in long-form question answering and machine translation tasks, employing <i>gpt-3.5-turbo</i> for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Motivation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <img src="static/images/training_signals.svg">
          <p>
            Human oversight capabilities are finite. Demonstrations or preferences get noisy when tasks become complicated. We ask, <b>how to get reliable and scalable supervision signals within limited human oversight capabilities?</b>
          </p>
          <p>
            As shown in this figure, the shadowed "superior area" better aligns with human preference, which is hard to reach for solely a noisy holistic reward. We propose to utilize multiple rewards hierarchically for more accurate and consistent supervision signals.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Long-form QA -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Task: Long-form Question Answering</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/long-form-QA-reward-selection.png">
        </div>
      </div>
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/long-form-QA-mean.png">
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <li>Through proactive reward selection, we choose the factuality reward for Long-form QA.</li>
            <li>Results show that <span class="dnerf">ALaRM</span> holds the best in both mean rewards and pairwise comparisons.</li>
          </p>

          <img src="static/images/long-form-QA-win-rate.png">
        </div>
      </div>
    </div>
    <!--/ Long-form QA -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- MT -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Task: Machine Translation</h2>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/MT-reward-selection.png">
        </div>
      </div>
      <div class="column is-two-fifths">
        <div class="content">
          <img src="static/images/MT-mean.png">
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <li>The same as in long-form QA, we choose the grammar reward by the results of reward selection.</li>
            <li>Again, <span class="dnerf">ALaRM</span> wins in pairwise comparisons and presents leading performance in mean rewards.</li>
          </p>

          <img src="static/images/MT-win-rate.png">
        </div>
      </div>
    </div>
    <!--/ MT -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Ablation Study. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation Study</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <img src="static/images/ablation.png" width="60%">
          </div>
          <p>
            While the main results should support the significance of <b>Combination</b> and <b>Hierarchical Structure</b>, we conduct extensive experiments to find out how <b>Reward Selection</b> affects <span class="dnerf">ALaRM</span>.
          </p>
          <p>
            As shown in the table, the proactively selected rewards present leading performance evaluated by both the holistic reward and <i>gpt-3.5-turbo</i>, demonstrating the effectiveness of <b>Reward Selection</b>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Ablation Study. -->
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/pdf/2403.06754.pdf">
        <i class="fas fa-file-pdf" style="color:white"></i>
      </a>
      <a class="icon-link" href="https://github.com/halfrot/ALaRM" class="external-link" disabled>
        <i class="fab fa-github" style="color:white"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
              code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
